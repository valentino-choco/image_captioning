{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8d3275",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa76ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import defaultdict\n",
    "\n",
    "import clip\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b5878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colour:\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "vscode_bg = '#1e1e1e'\n",
    "text_color = 'white'\n",
    "grid_color = '#444444'\n",
    "\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "plt.rcParams['font.monospace'] = ['Consolas', 'DejaVu Sans Mono', 'Courier New']\n",
    "plt.rcParams['figure.facecolor'] = vscode_bg\n",
    "plt.rcParams['axes.facecolor'] = vscode_bg\n",
    "plt.rcParams['axes.edgecolor'] = text_color\n",
    "plt.rcParams['axes.labelcolor'] = text_color\n",
    "plt.rcParams['xtick.color'] = text_color\n",
    "plt.rcParams['ytick.color'] = text_color\n",
    "plt.rcParams['text.color'] = text_color\n",
    "plt.rcParams['grid.color'] = grid_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8372b5a",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b6d123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Path to dataset files: /Users/valentin/.cache/kagglehub/datasets/eeshawn/flickr30k/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/valentin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"eeshawn/flickr30k\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb3f493",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a6441",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abef207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        # specials tokens\n",
    "        self.add_word('<pad>')\n",
    "        self.add_word('<unk>')\n",
    "        self.add_word('<start>')\n",
    "        self.add_word('<end>')\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "        return [self.word2idx.get(t, self.word2idx['<unk>']) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5bdadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data_pairs, vocab, cache_dir):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.vocab = vocab\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "    def __len__(self): return len(self.data_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, caption = self.data_pairs[idx]\n",
    "        filename = os.path.basename(img_path).split('.')[0] + \".pt\"\n",
    "        load_path = os.path.join(self.cache_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            cached = torch.load(load_path)\n",
    "            feat = cached['features'] if isinstance(cached, dict) else cached\n",
    "            if len(feat.shape) > 1: feat = feat[0]\n",
    "        except FileNotFoundError:\n",
    "            feat = torch.zeros(512) # fallback\n",
    "\n",
    "        tokens = self.vocab.encode(caption)\n",
    "        caption_indices = [self.vocab.word2idx['<start>']] + tokens + [self.vocab.word2idx['<end>']]\n",
    "        return feat, torch.tensor(caption_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aca8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, dataset_path, img_folder_name=\"flickr30k_images\", captions_filename=\"captions.txt\", cache_dir=\"cached_features\"):\n",
    "        self.root = dataset_path\n",
    "        self.img_folder = os.path.join(self.root, img_folder_name)\n",
    "        self.captions_file = self._find_file(captions_filename)\n",
    "        self.token_file = os.path.join(self.root, \"flickr30k.token.txt\")\n",
    "        self.cache_dir = cache_dir\n",
    "        self.vocab = Vocab()\n",
    "        self.data_pairs = []\n",
    "        \n",
    "    def _find_file(self, name):\n",
    "        for root, _, files in os.walk(self.root):\n",
    "            if name in files: return os.path.join(root, name)\n",
    "        return None\n",
    "\n",
    "    def prepare_data(self, limit=None, vocab_max_size=None):\n",
    "        \"\"\"Pipeline complet : CSV -> Token -> Vocab -> Pairs\"\"\"\n",
    "        if not os.path.exists(self.token_file):\n",
    "            self._convert_csv_to_token()\n",
    "            \n",
    "        with open(self.token_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        random.seed(42)\n",
    "        random.shuffle(lines)\n",
    "        \n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if limit and count >= limit: break\n",
    "            \n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2: continue\n",
    "            \n",
    "            img_id = parts[0].split('#')[0]\n",
    "            caption = parts[1]\n",
    "            img_path = os.path.join(self.img_folder, img_id)\n",
    "            \n",
    "            if os.path.exists(img_path):\n",
    "                self.data_pairs.append((img_path, caption))\n",
    "                for w in nltk.tokenize.word_tokenize(caption.lower()):\n",
    "                    if vocab_max_size and len(self.vocab) >= vocab_max_size: continue\n",
    "                    self.vocab.add_word(w)\n",
    "                count += 1\n",
    "                \n",
    "        print(f\"{Colour.GREEN}âœ… Data loaded:{Colour.END} {Colour.BOLD}{len(self.data_pairs)}{Colour.END} pairs | {Colour.BOLD}{len(self.vocab)}{Colour.END} vocab words\")\n",
    "\n",
    "    def _convert_csv_to_token(self):\n",
    "        try:\n",
    "            with open(self.captions_file, 'r', encoding='utf-8') as infile, \\\n",
    "                 open(self.token_file, 'w', encoding='utf-8') as outfile:\n",
    "                reader = csv.reader(infile)\n",
    "                next(reader)\n",
    "                for row in reader:\n",
    "                    if len(row) == 3:\n",
    "                        outfile.write(f\"{row[0]}#{row[1]}\\t{row[2]}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"{Colour.RED}âŒ Conversion error:{Colour.END} {e}\")\n",
    "    \n",
    "    def caption_collate_fn(self, batch):\n",
    "        imgs, caps = zip(*batch)\n",
    "        imgs = torch.stack(imgs)\n",
    "        caps_padded = pad_sequence(caps, batch_first=True, padding_value=0)\n",
    "        return imgs, caps_padded\n",
    "\n",
    "    def get_loaders(self, batch_size=64, test_size=0.2):\n",
    "        \"\"\"Divise par IMAGE et renvoie les DataLoaders\"\"\"\n",
    "        img_to_caps = {}\n",
    "        for img, cap in self.data_pairs:\n",
    "            if img not in img_to_caps: img_to_caps[img] = []\n",
    "            img_to_caps[img].append(cap)\n",
    "            \n",
    "        unique_imgs = list(img_to_caps.keys())\n",
    "        train_imgs, test_imgs = train_test_split(unique_imgs, test_size=test_size, random_state=42)\n",
    "        \n",
    "        train_pairs = [(img, cap) for img in train_imgs for cap in img_to_caps[img]]\n",
    "        test_pairs = [(img, cap) for img in test_imgs for cap in img_to_caps[img]]\n",
    "        \n",
    "        print(f\"{Colour.CYAN}ðŸ“Š Split:{Colour.END} Train {Colour.BOLD}{len(train_pairs)}{Colour.END} | Test {Colour.BOLD}{len(test_pairs)}{Colour.END}\")\n",
    "        \n",
    "        train_ds = CaptionDataset(train_pairs, self.vocab, self.cache_dir)\n",
    "        test_ds = CaptionDataset(test_pairs, self.vocab, self.cache_dir)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=self.caption_collate_fn, drop_last=True)\n",
    "        test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=self.caption_collate_fn, drop_last=True)\n",
    "        \n",
    "        return train_loader, test_loader, train_pairs, test_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a5772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mâœ… Data loaded:\u001b[0m \u001b[1m26000\u001b[0m pairs | \u001b[1m5000\u001b[0m vocab words\n",
      "\u001b[96mðŸ“Š Split:\u001b[0m Train \u001b[1m20855\u001b[0m | Test \u001b[1m5145\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager(path)\n",
    "data_manager.prepare_data(limit=26000, vocab_max_size=5000)\n",
    "train_loader, test_loader, train_pairs, test_pairs = data_manager.get_loaders(batch_size=100)\n",
    "vocab = data_manager.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1642b",
   "metadata": {},
   "source": [
    "# Part 1 : Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ea617",
   "metadata": {},
   "source": [
    "### alignment computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0077412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment(img_path, detector, model_align, preprocess_align, device, threshold=0.5):\n",
    "    img_pil = Image.open(img_path).convert(\"RGB\")\n",
    "    w_img, h_img = img_pil.size\n",
    "    img_tensor = T.ToTensor()(img_pil).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = detector([img_tensor])[0]\n",
    "        \n",
    "    keep_score = pred['scores'] > threshold\n",
    "    boxes = pred['boxes'][keep_score]\n",
    "    scores = pred['scores'][keep_score]\n",
    "    \n",
    "    keep_nms = torchvision.ops.nms(boxes, scores, 0.3)\n",
    "    final_boxes = boxes[keep_nms][:19]\n",
    "    crops_tensors = [preprocess_align(img_pil)]\n",
    "\n",
    "    for box in final_boxes:\n",
    "        x1, y1, x2, y2 = box.int().tolist()\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(w_img, x2), min(h_img, y2)\n",
    "        if x2 - x1 < 1 or y2 - y1 < 1:\n",
    "            continue\n",
    "        crop = img_pil.crop((x1, y1, x2, y2))\n",
    "        crops_tensors.append(preprocess_align(crop))\n",
    "\n",
    "    batch_input = torch.stack(crops_tensors).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_features = model_align.encode_image(batch_input)\n",
    "    \n",
    "    regions_features = [feat.cpu().float().numpy().flatten() for feat in batch_features]\n",
    "    return regions_features, final_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4df07561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionDataGenerator:\n",
    "    def __init__(self, data_pairs, vocab, detector, model_align, preprocess_align, device):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.vocab = vocab\n",
    "        self.detector = detector\n",
    "        self.model_align = model_align\n",
    "        self.preprocess = preprocess_align\n",
    "        self.device = device\n",
    "        \n",
    "    def generate_region_dataset(self, limit=100):\n",
    "        region_training_data = []\n",
    "        \n",
    "        print(f\"{Colour.CYAN}ðŸš€ Generating region data from alignment...{Colour.END}\")\n",
    "        subset = self.data_pairs[:limit] \n",
    "        \n",
    "        for img_path, caption in tqdm.tqdm(subset):\n",
    "            try:\n",
    "                region_feats, boxes = get_alignment(img_path, self.detector, self.model_align, self.preprocess, self.device)\n",
    "                \n",
    "                if len(region_feats) == 0: continue\n",
    "\n",
    "                words = nltk.word_tokenize(caption.lower())\n",
    "                stop_words = {'.', ',', 'a', 'the', 'is', 'are', 'in', 'on', 'of', 'and', 'with', 'to', 'at'}\n",
    "                meaningful_words = [w for w in words if w not in stop_words and w in self.vocab.word2idx]\n",
    "                \n",
    "                if not meaningful_words: continue\n",
    "                img_features_tensor = torch.tensor(np.array(region_feats)).to(self.device)\n",
    "                img_features_tensor /= img_features_tensor.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                text_tokens = clip.tokenize(meaningful_words).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    text_features = self.model_align.encode_text(text_tokens)\n",
    "                    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "                similarity = (100.0 * img_features_tensor @ text_features.T).softmax(dim=0)\n",
    "                best_region_indices = similarity.argmax(dim=0).cpu().numpy()\n",
    "                \n",
    "                for i, word in enumerate(meaningful_words):\n",
    "                    region_idx = best_region_indices[i]\n",
    "                    best_feat = region_feats[region_idx]\n",
    "                    word_idx = self.vocab.word2idx[word]\n",
    "                    region_training_data.append((best_feat, word_idx))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "                \n",
    "        print(f\"{Colour.GREEN}âœ… Generated {len(region_training_data)} region-word pairs.{Colour.END}\")\n",
    "        return region_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757582d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionDataset(Dataset):\n",
    "    def __init__(self, region_data):\n",
    "        self.data = region_data\n",
    "        \n",
    "    def __len__(self): return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feat, word_idx = self.data[idx]\n",
    "        feat_tensor = torch.tensor(feat, dtype=torch.float)\n",
    "        caption_indices = [vocab.word2idx['<start>'], word_idx, vocab.word2idx['<end>']]\n",
    "        return feat_tensor, torch.tensor(caption_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695ea73",
   "metadata": {},
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e323b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alignment(img_path, caption, detector, model_align, preprocess_align, device, threshold=0.5, dist_test=20):\n",
    "    region_feats, boxes = get_alignment(img_path, detector, model_align, preprocess_align, device, threshold)\n",
    "    \n",
    "    if not boxes.shape[0]:\n",
    "        print(f\"{Colour.RED}no objects detected in image.{Colour.END}\")\n",
    "        return\n",
    "\n",
    "    img_features = torch.tensor(np.array(region_feats)).to(device)\n",
    "    img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    words = nltk.word_tokenize(caption.lower())\n",
    "    stop_words = {'.', ',', 'a', 'the', 'is', 'are', 'in', 'on', 'of', 'and', 'with', 'to', 'at'}\n",
    "    meaningful_words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "    if not meaningful_words:\n",
    "        print(f\"{Colour.YELLOW}no meaningful words found after filtering.{Colour.END}\")\n",
    "        return\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize(meaningful_words).to(device)\n",
    "        text_features = model_align.encode_text(text_tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (100.0 * img_features @ text_features.T).softmax(dim=0).cpu().numpy()\n",
    "    best_indices = similarity.argmax(axis=0)\n",
    "    best_scores = similarity.max(axis=0)\n",
    "\n",
    "    img_pil = Image.open(img_path).convert(\"RGB\")\n",
    "    w_img, h_img = img_pil.size\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8), facecolor=vscode_bg)\n",
    "    ax.set_facecolor(vscode_bg)\n",
    "\n",
    "    ax.imshow(img_pil)\n",
    "    \n",
    "    colors = cm.tab20(range(len(meaningful_words)))\n",
    "    text_x_start = w_img + (w_img * 0.05)\n",
    "\n",
    "    for i, (word, best_idx, score) in enumerate(zip(meaningful_words, best_indices, best_scores)):\n",
    "        color = colors[i]\n",
    "        \n",
    "        if best_idx == 0:\n",
    "            target_x, target_y = w_img / 2, h_img / 2\n",
    "        else:\n",
    "            box_idx = best_idx - 1 \n",
    "            box = boxes[box_idx].cpu().numpy() if isinstance(boxes[box_idx], torch.Tensor) else boxes[box_idx]\n",
    "            x1, y1, x2, y2 = box\n",
    "            \n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2.5, edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            target_x, target_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        text_y = (i * dist_test) + (h_img * 0.05)\n",
    "        label_text = f\"{word} ({score:.2f})\"\n",
    "        ax.text(text_x_start, text_y, label_text, \n",
    "                fontsize=13, fontweight='bold', color='black', \n",
    "                va='center', ha='left', family='monospace',\n",
    "                bbox=dict(boxstyle='square,pad=0.4', facecolor=color, edgecolor='none', alpha=0.9))\n",
    "\n",
    "        if score > 0.1: # threshold to draw arrow\n",
    "            ax.annotate('', \n",
    "                        xy=(target_x, target_y), \n",
    "                        xytext=(text_x_start, text_y),\n",
    "                        arrowprops=dict(arrowstyle='->', color=color, linewidth=2, mutation_scale=15))\n",
    "\n",
    "    ax.set_xlim(0, w_img * 1.6) \n",
    "    ax.set_ylim(h_img, 0) # flip Y to match image coordinates\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02ff66",
   "metadata": {},
   "source": [
    "### rcnn model & CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97181788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mâœ… Models loaded\u001b[0m\n",
      "ðŸ’» Device: \u001b[93mCPU\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_align, preprocess_align = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "model_align.eval()\n",
    "\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "resnet = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights).to(DEVICE)   # pretrained=True\n",
    "resnet.eval()\n",
    "\n",
    "print(f\"{Colour.GREEN}âœ… Models loaded{Colour.END}\")\n",
    "print(f\"ðŸ’» Device: {Colour.YELLOW}{DEVICE.upper()}{Colour.END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21447a",
   "metadata": {},
   "source": [
    "# Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b898d9b",
   "metadata": {},
   "source": [
    "### captioning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255895d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, image_feature_dim, hidden_size, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.image_feature_dim = image_feature_dim\n",
    "        self.image_projection = nn.Linear(image_feature_dim, embed_dim)\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim + embed_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, image_features, captions):\n",
    "        img_emb = self.image_projection(image_features) # [Batch, Embed_Dim]\n",
    "        img_emb = F.relu(img_emb)\n",
    "        \n",
    "        word_embeds = self.word_embedding(captions)     # [Batch, Seq_Len, Embed_Dim]\n",
    "        word_embeds = self.dropout(word_embeds)\n",
    "        \n",
    "        seq_len = word_embeds.size(1)\n",
    "        img_emb_expanded = img_emb.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        \n",
    "        lstm_inputs = torch.cat((word_embeds, img_emb_expanded), dim=2) \n",
    "        lstm_out, _ = self.lstm(lstm_inputs)\n",
    "        \n",
    "        return self.output_projection(lstm_out)\n",
    "\n",
    "    def sample(self, image_features, vocab, max_len=20, temperature=1.0, device='cpu'):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            img_emb = self.image_projection(image_features)\n",
    "            img_emb = F.relu(img_emb)\n",
    "            img_emb = img_emb.unsqueeze(1) \n",
    "            \n",
    "            hidden = None \n",
    "            start_token = vocab.word2idx.get('<start>', vocab.word2idx.get('<unk>', 1))\n",
    "            current_input_idx = torch.tensor([start_token]).long().to(device).unsqueeze(0) # [1, 1]\n",
    "            \n",
    "            generated_ids = []\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                word_emb = self.word_embedding(current_input_idx) # [1, 1, Embed_Dim]\n",
    "\n",
    "                lstm_input = torch.cat((word_emb, img_emb), dim=2) # [1, 1, Embed_Dim * 2]\n",
    "                lstm_out, hidden = self.lstm(lstm_input, hidden)\n",
    "                outputs = self.output_projection(lstm_out) # [1, 1, Vocab]\n",
    "                \n",
    "                probs = F.softmax(outputs[0, 0] / temperature, dim=0)\n",
    "                \n",
    "                if temperature == 0:\n",
    "                    next_word_idx = torch.argmax(probs).item()\n",
    "                else:\n",
    "                    next_word_idx = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                if next_word_idx == vocab.word2idx.get('<end>', 0):\n",
    "                    break\n",
    "                \n",
    "                generated_ids.append(next_word_idx)\n",
    "                current_input_idx = torch.tensor([next_word_idx]).long().to(device).unsqueeze(0)\n",
    "                \n",
    "            tokens = [vocab.idx2word.get(idx, '<unk>') for idx in generated_ids]\n",
    "            return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befba238",
   "metadata": {},
   "source": [
    "### train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18639baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_captioning_epoch(dataloader, model_captioning, optimizer, criterion, device, clip_threshold=5.0):\n",
    "    model_captioning.train()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    pbar = tqdm.tqdm(dataloader, desc=\"Training Captioning\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        img_feats, captions = batch\n",
    "        img_feats = img_feats.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        targets = captions[:, 1:] # Tout sauf le premier (<start>)\n",
    "        inputs = captions[:, :-1] # Tout sauf le dernier (<end> ou padding)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model_captioning(img_feats, inputs) # [Batch, Seq_Len-1, Vocab]\n",
    "        \n",
    "        outputs_flat = outputs.reshape(-1, outputs.shape[-1])\n",
    "        targets_flat = targets.reshape(-1)\n",
    "        \n",
    "        loss = criterion(outputs_flat, targets_flat)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_captioning.parameters(), clip_threshold)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    return total_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bef006dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 512\n",
    "IMG_FEAT_DIM = 512\n",
    "HIDDEN_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e77ad3",
   "metadata": {},
   "source": [
    "### To ease the demo, we will not do the first part of training here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed188c",
   "metadata": {},
   "source": [
    "### precompute features and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d56b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cache_region_data(data_pairs, vocab, detector, model_align, preprocess_align, device, save_path=\"cached_features/region_dataset_cache.pt\", limit=None):\n",
    "#     if os.path.exists(save_path):\n",
    "#         print(f\"{Colour.GREEN}âœ… Region dataset found at {Colour.BOLD}{save_path}{Colour.END}\")\n",
    "#         return torch.load(save_path, weights_only=False)\n",
    "\n",
    "#     print(f\"{Colour.CYAN}ðŸš€ Generating and caching region data (This happens only once)...{Colour.END}\")\n",
    "#     region_training_data = []\n",
    "    \n",
    "#     subset = data_pairs[:limit] if limit else data_pairs\n",
    "#     pbar = tqdm.tqdm(subset)\n",
    "    \n",
    "#     for img_path, caption in pbar:\n",
    "#         try:\n",
    "#             region_feats, boxes = get_alignment(img_path, detector, model_align, preprocess_align, device)\n",
    "            \n",
    "#             if len(region_feats) == 0: continue\n",
    "\n",
    "#             words = nltk.word_tokenize(caption.lower())\n",
    "#             stop_words = {'.', ',', 'a', 'the', 'is', 'are', 'in', 'on', 'of', 'and', 'with', 'to', 'at'}\n",
    "#             meaningful_words = [w for w in words if w not in stop_words and w in vocab.word2idx]\n",
    "            \n",
    "#             if not meaningful_words: continue\n",
    "\n",
    "#             text_tokens = clip.tokenize(meaningful_words).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 text_features = model_align.encode_text(text_tokens)\n",
    "#                 text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "#             img_features_tensor = torch.tensor(np.array(region_feats)).to(device).to(text_features.dtype)\n",
    "#             img_features_tensor /= img_features_tensor.norm(dim=-1, keepdim=True)\n",
    "\n",
    "#             similarity = (100.0 * img_features_tensor @ text_features.T).softmax(dim=0)\n",
    "#             best_region_indices = similarity.argmax(dim=0).cpu().numpy()\n",
    "            \n",
    "#             for i, word in enumerate(meaningful_words):\n",
    "#                 region_idx = best_region_indices[i]\n",
    "#                 best_feat = region_feats[region_idx] \n",
    "#                 word_idx = vocab.word2idx[word]\n",
    "#                 region_training_data.append((best_feat, word_idx))\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             continue\n",
    "    \n",
    "#     print(f\"{Colour.GREEN}ðŸ’¾ Saving {len(region_training_data)} pairs to {save_path}...{Colour.END}\")\n",
    "#     torch.save(region_training_data, save_path)\n",
    "#     return region_training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd553786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_data = cache_region_data(train_pairs, vocab, resnet, model_align, preprocess_align, DEVICE, limit=26000)\n",
    "# region_ds = RegionDataset(region_data)\n",
    "# region_loader = DataLoader(region_ds, batch_size=100, shuffle=True, collate_fn=data_manager.caption_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da686c1e",
   "metadata": {},
   "source": [
    "### do we have a pretrained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8171962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_captioning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f143a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_FEAT_DIM_REGION = 512\n",
    "\n",
    "# # that part is done only if we want to train from scratch\n",
    "# if not pretrained_model_captioning:\n",
    "#     print(f\"{Colour.CYAN}ðŸš€ STEP 1: Setting up Region Model (CLIP features)...{Colour.END}\")\n",
    "#     model_captioning_region = CaptioningModel(VOCAB_SIZE, EMBED_DIM, IMG_FEAT_DIM_REGION, HIDDEN_SIZE).to(DEVICE)\n",
    "#     optimizer_captioning_region = torch.optim.RMSprop(model_captioning_region.parameters(), lr=1e-4)\n",
    "#     train_losses_region = []\n",
    "#     epoch_region = 0\n",
    "#     criterion_captioning_region = nn.CrossEntropyLoss(ignore_index=vocab.word2idx.get('<pad>', 0))\n",
    "#     print(f\"{Colour.GREEN}âœ… REGION Model initialized (Input: 512){Colour.END}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c03967ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # that part is done only if we want to train from scratch\n",
    "# if not pretrained_model_captioning:\n",
    "#     print(f\"{Colour.CYAN}ðŸš€ Starting Region Training on {Colour.YELLOW}{DEVICE.upper()}{Colour.END}\")\n",
    "\n",
    "#     epoch_max_region = 1\n",
    "#     # epoch_max_region = 20\n",
    "\n",
    "#     while epoch_region < epoch_max_region:\n",
    "#         print(f\"\\n{Colour.BOLD}=== Region Epoch {epoch_region+1}/{epoch_max_region} ==={Colour.END}\")\n",
    "        \n",
    "#         avg_loss_region = train_one_captioning_epoch(region_loader, model_captioning_region, optimizer_captioning_region, criterion_captioning_region, device=DEVICE, clip_threshold=5.0)\n",
    "#         train_losses_region.append(avg_loss_region)\n",
    "        \n",
    "#         print(f\"{Colour.GREEN}ðŸ“‰ Avg Region Loss: {Colour.BOLD}{avg_loss_region:.4f}{Colour.END}\")\n",
    "#         epoch_region += 1\n",
    "\n",
    "#     print(f\"\\n{Colour.GREEN}âœ… Region Training complete! The LSTM now knows the vocabulary.{Colour.END}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ea8dc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mâœ… Transfer complete! Model is ready for full sentences.\u001b[0m\n",
      "\u001b[92mâœ… REGION Model initialized (Input: 512)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if pretrained_model_captioning:\n",
    "    file_saved_model = 'saved_models/captioning_model_epoch_60.pth'\n",
    "    checkpoint = torch.load(file_saved_model, map_location=DEVICE)\n",
    "    model_captioning = CaptioningModel(VOCAB_SIZE, EMBED_DIM, IMG_FEAT_DIM, HIDDEN_SIZE).to(DEVICE)\n",
    "    model_captioning.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer_captioning = torch.optim.RMSprop(model_captioning.parameters(), lr=1e-4)\n",
    "    optimizer_captioning.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    vocab.word2idx = checkpoint['vocab_word2idx']\n",
    "    vocab.idx2word = checkpoint['vocab_idx2word']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"{Colour.GREEN}âœ… Pretrained model loaded.{Colour.END}\")\n",
    "else:\n",
    "    model_captioning = CaptioningModel(VOCAB_SIZE, EMBED_DIM, IMG_FEAT_DIM, HIDDEN_SIZE).to(DEVICE)\n",
    "\n",
    "    # # greffe\n",
    "    # region_state = model_captioning_region.state_dict()\n",
    "    # full_state = model_captioning.state_dict()\n",
    "    # region_state = model_captioning_region.state_dict()\n",
    "    # full_state = model_captioning.state_dict()\n",
    "\n",
    "    # pretrained_dict = {k: v for k, v in region_state.items() if k in full_state and 'image_projection' not in k}\n",
    "\n",
    "    # full_state.update(pretrained_dict)\n",
    "    # model_captioning.load_state_dict(full_state)\n",
    "\n",
    "    print(f\"{Colour.GREEN}âœ… Transfer complete! Model is ready for full sentences.{Colour.END}\")\n",
    "\n",
    "    optimizer_captioning = torch.optim.RMSprop(model_captioning.parameters(), lr=1e-4)\n",
    "    train_losses = []\n",
    "    epoch = 0\n",
    "    print(f\"{Colour.GREEN}âœ… REGION Model initialized (Input: 512){Colour.END}\")\n",
    "\n",
    "criterion_captioning = nn.CrossEntropyLoss(ignore_index=vocab.word2idx.get('<pad>', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d591c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mðŸš€ Starting Full-Image Training on \u001b[93mCPU\u001b[0m\n",
      "\n",
      "\u001b[1m=== Full Epoch 1/3 ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.3300: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208/208 [00:37<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mðŸ“‰ Avg Loss: \u001b[1m4.7792\u001b[0m\n",
      "\n",
      "\u001b[1m=== Full Epoch 2/3 ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.0669: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208/208 [00:38<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mðŸ“‰ Avg Loss: \u001b[1m4.2147\u001b[0m\n",
      "\n",
      "\u001b[1m=== Full Epoch 3/3 ===\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.9844: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208/208 [00:40<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mðŸ“‰ Avg Loss: \u001b[1m4.0232\u001b[0m\n",
      "\n",
      "\u001b[92mâœ… All training complete! You can now visualize.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"saved_models\"):\n",
    "    os.makedirs(\"saved_models\")\n",
    "\n",
    "epoch_max = 3\n",
    "# epoch_max = 100\n",
    "\n",
    "print(f\"{Colour.CYAN}ðŸš€ Starting Full-Image Training on {Colour.YELLOW}{DEVICE.upper()}{Colour.END}\")\n",
    "\n",
    "while epoch < epoch_max:\n",
    "    print(f\"\\n{Colour.BOLD}=== Full Epoch {epoch+1}/{epoch_max} ==={Colour.END}\")\n",
    "    avg_loss = train_one_captioning_epoch(train_loader, model_captioning, optimizer_captioning, criterion_captioning, device=DEVICE, clip_threshold=5.0)    \n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if len(train_losses) <= 1:\n",
    "        print(f\"{Colour.GREEN}ðŸ“‰ Avg Loss: {Colour.BOLD}{avg_loss:.4f}{Colour.END}\")\n",
    "    elif train_losses[-2] > train_losses[-1]:\n",
    "        print(f\"{Colour.GREEN}ðŸ“‰ Avg Loss: {Colour.BOLD}{avg_loss:.4f}{Colour.END}\")\n",
    "    else:\n",
    "        print(f\"{Colour.RED}ðŸ“‰ Avg Loss: {Colour.BOLD}{avg_loss:.4f}{Colour.END} (did not improve)\")\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({\n",
    "        'model_state_dict': model_captioning.state_dict(),\n",
    "        'optimizer_state_dict': optimizer_captioning.state_dict(),\n",
    "        'vocab_word2idx': vocab.word2idx,\n",
    "        'vocab_idx2word': vocab.idx2word,\n",
    "        'train_losses': train_losses,\n",
    "        'epoch': epoch\n",
    "    }, f'saved_models/captioning_model_epoch_{epoch}.pth')\n",
    "        \n",
    "    epoch += 1\n",
    "\n",
    "print(f\"\\n{Colour.GREEN}âœ… All training complete! You can now visualize.{Colour.END}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ee6c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(train_losses):\n",
    "    if not train_losses:\n",
    "        print(f\"{Colour.YELLOW}[WARNING] No training loss data to plot.{Colour.END}\")\n",
    "        return\n",
    "\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6), facecolor=vscode_bg)\n",
    "    ax.set_facecolor(vscode_bg)\n",
    "    line_color = \"#0b9220\"\n",
    "    ax.plot(epochs, train_losses, marker='o', \n",
    "            linewidth=2, \n",
    "            markersize=6, \n",
    "            color=line_color, \n",
    "            markerfacecolor=line_color,\n",
    "            label='Train Loss')\n",
    "\n",
    "    ax.fill_between(epochs, train_losses, alpha=0.3, color=line_color)\n",
    "    plt.xlabel('Epoch', fontsize=12, color=text_color, family='monospace')\n",
    "    plt.ylabel('Loss', fontsize=12, color=text_color, family='monospace')\n",
    "    plt.title('Training Loss Over Epochs', fontsize=14, color=text_color, pad=15, family='monospace')\n",
    "\n",
    "    ax.tick_params(axis='both', colors=text_color, labelsize=10, length=6, width=1.2)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_color(text_color)\n",
    "    ax.spines['left'].set_linewidth(1.2)\n",
    "    ax.spines['bottom'].set_color(text_color)\n",
    "    ax.spines['bottom'].set_linewidth(1.2)\n",
    "\n",
    "    ax.grid(True, which='major', linestyle='--', linewidth=0.8, alpha=0.2, color='white')\n",
    "\n",
    "    if len(train_losses) <= 20:\n",
    "        ax.set_xticks(epochs)\n",
    "    else:\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    ax.set_xlim(1, max(epochs) if len(epochs) > 0 else 1)\n",
    "    ax.set_ylim(0, None)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10f7e1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_training_loss(train_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
