{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8d3275",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa76ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import defaultdict\n",
    "\n",
    "import clip\n",
    "import nltk\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b5878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colour:\n",
    "    BLUE = '\\033[94m'\n",
    "    CYAN = '\\033[96m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "vscode_bg = '#1e1e1e'\n",
    "text_color = 'white'\n",
    "grid_color = '#444444'\n",
    "\n",
    "plt.rcParams['font.family'] = 'monospace'\n",
    "plt.rcParams['font.monospace'] = ['Consolas', 'DejaVu Sans Mono', 'Courier New']\n",
    "plt.rcParams['figure.facecolor'] = vscode_bg\n",
    "plt.rcParams['axes.facecolor'] = vscode_bg\n",
    "plt.rcParams['axes.edgecolor'] = text_color\n",
    "plt.rcParams['axes.labelcolor'] = text_color\n",
    "plt.rcParams['xtick.color'] = text_color\n",
    "plt.rcParams['ytick.color'] = text_color\n",
    "plt.rcParams['text.color'] = text_color\n",
    "plt.rcParams['grid.color'] = grid_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8372b5a",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b6d123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.1).\n",
      "Path to dataset files: /Users/valentin/.cache/kagglehub/datasets/eeshawn/flickr30k/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/valentin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"eeshawn/flickr30k\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699bd16",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a6441",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abef207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        # specials tokens\n",
    "        self.add_word('<pad>')\n",
    "        self.add_word('<unk>')\n",
    "        self.add_word('<start>')\n",
    "        self.add_word('<end>')\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "        return [self.word2idx.get(t, self.word2idx['<unk>']) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5bdadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, data_pairs, vocab, cache_dir):\n",
    "        self.data_pairs = data_pairs\n",
    "        self.vocab = vocab\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "    def __len__(self): return len(self.data_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, caption = self.data_pairs[idx]\n",
    "        filename = os.path.basename(img_path).split('.')[0] + \".pt\"\n",
    "        load_path = os.path.join(self.cache_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            cached = torch.load(load_path)\n",
    "            feat = cached['features'] if isinstance(cached, dict) else cached\n",
    "            if len(feat.shape) > 1: feat = feat[0]\n",
    "        except FileNotFoundError:\n",
    "            feat = torch.zeros(512) # fallback\n",
    "\n",
    "        tokens = self.vocab.encode(caption)\n",
    "        caption_indices = [self.vocab.word2idx['<start>']] + tokens + [self.vocab.word2idx['<end>']]\n",
    "        return feat, torch.tensor(caption_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aca8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, dataset_path, img_folder_name=\"flickr30k_images\", captions_filename=\"captions.txt\", cache_dir=\"cached_features\"):\n",
    "        self.root = dataset_path\n",
    "        self.img_folder = os.path.join(self.root, img_folder_name)\n",
    "        self.captions_file = self._find_file(captions_filename)\n",
    "        self.token_file = os.path.join(self.root, \"flickr30k.token.txt\")\n",
    "        self.cache_dir = cache_dir\n",
    "        self.vocab = Vocab()\n",
    "        self.data_pairs = []\n",
    "        \n",
    "    def _find_file(self, name):\n",
    "        for root, _, files in os.walk(self.root):\n",
    "            if name in files: return os.path.join(root, name)\n",
    "        return None\n",
    "\n",
    "    def prepare_data(self, limit=None, vocab_max_size=None):\n",
    "        \"\"\"Pipeline complet : CSV -> Token -> Vocab -> Pairs\"\"\"\n",
    "        if not os.path.exists(self.token_file):\n",
    "            self._convert_csv_to_token()\n",
    "            \n",
    "        with open(self.token_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        random.seed(42)\n",
    "        random.shuffle(lines)\n",
    "        \n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if limit and count >= limit: break\n",
    "            \n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2: continue\n",
    "            \n",
    "            img_id = parts[0].split('#')[0]\n",
    "            caption = parts[1]\n",
    "            img_path = os.path.join(self.img_folder, img_id)\n",
    "            \n",
    "            if os.path.exists(img_path):\n",
    "                self.data_pairs.append((img_path, caption))\n",
    "                for w in nltk.tokenize.word_tokenize(caption.lower()):\n",
    "                    if vocab_max_size and len(self.vocab) >= vocab_max_size: continue\n",
    "                    self.vocab.add_word(w)\n",
    "                count += 1\n",
    "                \n",
    "        print(f\"{Colour.GREEN}‚úÖ Data loaded:{Colour.END} {Colour.BOLD}{len(self.data_pairs)}{Colour.END} pairs | {Colour.BOLD}{len(self.vocab)}{Colour.END} vocab words\")\n",
    "\n",
    "    def _convert_csv_to_token(self):\n",
    "        try:\n",
    "            with open(self.captions_file, 'r', encoding='utf-8') as infile, \\\n",
    "                 open(self.token_file, 'w', encoding='utf-8') as outfile:\n",
    "                reader = csv.reader(infile)\n",
    "                next(reader)\n",
    "                for row in reader:\n",
    "                    if len(row) == 3:\n",
    "                        outfile.write(f\"{row[0]}#{row[1]}\\t{row[2]}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"{Colour.RED}‚ùå Conversion error:{Colour.END} {e}\")\n",
    "    \n",
    "    def caption_collate_fn(self, batch):\n",
    "        imgs, caps = zip(*batch)\n",
    "        imgs = torch.stack(imgs)\n",
    "        caps_padded = pad_sequence(caps, batch_first=True, padding_value=0)\n",
    "        return imgs, caps_padded\n",
    "\n",
    "    def get_loaders(self, batch_size=64, test_size=0.2):\n",
    "        \"\"\"Divise par IMAGE et renvoie les DataLoaders\"\"\"\n",
    "        img_to_caps = {}\n",
    "        for img, cap in self.data_pairs:\n",
    "            if img not in img_to_caps: img_to_caps[img] = []\n",
    "            img_to_caps[img].append(cap)\n",
    "            \n",
    "        unique_imgs = list(img_to_caps.keys())\n",
    "        train_imgs, test_imgs = train_test_split(unique_imgs, test_size=test_size, random_state=42)\n",
    "        \n",
    "        train_pairs = [(img, cap) for img in train_imgs for cap in img_to_caps[img]]\n",
    "        test_pairs = [(img, cap) for img in test_imgs for cap in img_to_caps[img]]\n",
    "        \n",
    "        print(f\"{Colour.CYAN}üìä Split:{Colour.END} Train {Colour.BOLD}{len(train_pairs)}{Colour.END} | Test {Colour.BOLD}{len(test_pairs)}{Colour.END}\")\n",
    "        \n",
    "        train_ds = CaptionDataset(train_pairs, self.vocab, self.cache_dir)\n",
    "        test_ds = CaptionDataset(test_pairs, self.vocab, self.cache_dir)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=self.caption_collate_fn, drop_last=True)\n",
    "        test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=self.caption_collate_fn, drop_last=True)\n",
    "        \n",
    "        return train_loader, test_loader, train_pairs, test_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a5772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m‚úÖ Data loaded:\u001b[0m \u001b[1m26000\u001b[0m pairs | \u001b[1m5000\u001b[0m vocab words\n",
      "\u001b[96müìä Split:\u001b[0m Train \u001b[1m20855\u001b[0m | Test \u001b[1m5145\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_manager = DataManager(path)\n",
    "data_manager.prepare_data(limit=26000, vocab_max_size=5000)\n",
    "train_loader, test_loader, train_pairs, test_pairs = data_manager.get_loaders(batch_size=100)\n",
    "vocab = data_manager.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f1e9f",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ea617",
   "metadata": {},
   "source": [
    "### alignment computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7d3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alignment(img_path, detector, model_align, preprocess_align, device, threshold=0.5):\n",
    "    img_pil = Image.open(img_path).convert(\"RGB\")\n",
    "    w_img, h_img = img_pil.size\n",
    "    img_tensor = T.ToTensor()(img_pil).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = detector([img_tensor])[0]\n",
    "        \n",
    "    keep_score = pred['scores'] > threshold\n",
    "    boxes = pred['boxes'][keep_score]\n",
    "    scores = pred['scores'][keep_score]\n",
    "    \n",
    "    keep_nms = torchvision.ops.nms(boxes, scores, 0.3)\n",
    "    final_boxes = boxes[keep_nms][:19]\n",
    "    crops_tensors = [preprocess_align(img_pil)]\n",
    "\n",
    "    for box in final_boxes:\n",
    "        x1, y1, x2, y2 = box.int().tolist()\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(w_img, x2), min(h_img, y2)\n",
    "        if x2 - x1 < 1 or y2 - y1 < 1:\n",
    "            continue\n",
    "        crop = img_pil.crop((x1, y1, x2, y2))\n",
    "        crops_tensors.append(preprocess_align(crop))\n",
    "\n",
    "    batch_input = torch.stack(crops_tensors).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_features = model_align.encode_image(batch_input)\n",
    "    \n",
    "    regions_features = [feat.cpu().float().numpy().flatten() for feat in batch_features]\n",
    "    return regions_features, final_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695ea73",
   "metadata": {},
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e323b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_alignment(img_path, caption, detector, model_align, preprocess_align, device, threshold=0.5, dist_test=20):\n",
    "    region_feats, boxes = get_alignment(img_path, detector, model_align, preprocess_align, device, threshold)\n",
    "    \n",
    "    if not boxes.shape[0]:\n",
    "        print(f\"{Colour.RED}no objects detected in image.{Colour.END}\")\n",
    "        return\n",
    "\n",
    "    img_features = torch.tensor(np.array(region_feats)).to(device)\n",
    "    img_features /= img_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    words = nltk.word_tokenize(caption.lower())\n",
    "    stop_words = {'.', ',', 'a', 'the', 'is', 'are', 'in', 'on', 'of', 'and', 'with', 'to', 'at'}\n",
    "    meaningful_words = [w for w in words if w not in stop_words]\n",
    "    meaningful_words.reverse()      # test\n",
    "    \n",
    "    if not meaningful_words:\n",
    "        print(f\"{Colour.YELLOW}no meaningful words found after filtering.{Colour.END}\")\n",
    "        return\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_tokens = clip.tokenize(meaningful_words).to(device)\n",
    "        text_features = model_align.encode_text(text_tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (100.0 * img_features @ text_features.T).softmax(dim=0).cpu().numpy()\n",
    "    best_indices = similarity.argmax(axis=0)\n",
    "    best_scores = similarity.max(axis=0)\n",
    "\n",
    "    img_pil = Image.open(img_path).convert(\"RGB\")\n",
    "    w_img, h_img = img_pil.size\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8), facecolor=vscode_bg)\n",
    "    ax.set_facecolor(vscode_bg)\n",
    "\n",
    "    ax.imshow(img_pil)\n",
    "    \n",
    "    colors = cm.tab20(range(len(meaningful_words)))\n",
    "    text_x_start = w_img + (w_img * 0.05)\n",
    "\n",
    "    for i, (word, best_idx, score) in enumerate(zip(meaningful_words, best_indices, best_scores)):\n",
    "        color = colors[i]\n",
    "        \n",
    "        if best_idx == 0:\n",
    "            target_x, target_y = w_img / 2, h_img / 2\n",
    "        else:\n",
    "            box_idx = best_idx - 1 \n",
    "            box = boxes[box_idx].cpu().numpy() if isinstance(boxes[box_idx], torch.Tensor) else boxes[box_idx]\n",
    "            x1, y1, x2, y2 = box\n",
    "            \n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2.5, edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            target_x, target_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        text_y = (i * dist_test) + (h_img * 0.05)\n",
    "        label_text = f\"{word} ({score:.2f})\"\n",
    "        ax.text(text_x_start, text_y, label_text, \n",
    "                fontsize=13, fontweight='bold', color='black', \n",
    "                va='center', ha='left', family='monospace',\n",
    "                bbox=dict(boxstyle='square,pad=0.4', facecolor=color, edgecolor='none', alpha=0.9))\n",
    "\n",
    "        if score > 0.1: # threshold to draw arrow\n",
    "            ax.annotate('', \n",
    "                        xy=(target_x, target_y), \n",
    "                        xytext=(text_x_start, text_y),\n",
    "                        arrowprops=dict(arrowstyle='->', color=color, linewidth=2, mutation_scale=15))\n",
    "\n",
    "    ax.set_xlim(0, w_img * 1.6) \n",
    "    ax.set_ylim(h_img, 0) # flip Y to match image coordinates\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97181788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m‚úÖ Models loaded\u001b[0m\n",
      "üíª Device: \u001b[93mCPU\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_align, preprocess_align = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "model_align.eval()\n",
    "\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "resnet = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights).to(DEVICE)   # pretrained=True\n",
    "resnet.eval()\n",
    "\n",
    "print(f\"{Colour.GREEN}‚úÖ Models loaded{Colour.END}\")\n",
    "print(f\"üíª Device: {Colour.YELLOW}{DEVICE.upper()}{Colour.END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21447a",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255895d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, image_feature_dim, hidden_size, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.image_projection = nn.Linear(image_feature_dim, embed_dim)\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim + embed_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.output_projection = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, image_features, captions):\n",
    "        img_emb = self.image_projection(image_features) # [Batch, Embed_Dim]\n",
    "        img_emb = F.relu(img_emb)\n",
    "        \n",
    "        word_embeds = self.word_embedding(captions)     # [Batch, Seq_Len, Embed_Dim]\n",
    "        word_embeds = self.dropout(word_embeds)\n",
    "        \n",
    "        seq_len = word_embeds.size(1)\n",
    "        img_emb_expanded = img_emb.unsqueeze(1).expand(-1, seq_len, -1)\n",
    "        \n",
    "        lstm_inputs = torch.cat((word_embeds, img_emb_expanded), dim=2) \n",
    "        lstm_out, _ = self.lstm(lstm_inputs)\n",
    "        \n",
    "        return self.output_projection(lstm_out)\n",
    "\n",
    "    def sample(self, image_features, vocab, max_len=20, temperature=1.0, device='cpu'):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            img_emb = self.image_projection(image_features)\n",
    "            img_emb = F.relu(img_emb)\n",
    "            img_emb = img_emb.unsqueeze(1) \n",
    "            \n",
    "            hidden = None \n",
    "            start_token = vocab.word2idx.get('<start>', vocab.word2idx.get('<unk>', 1))\n",
    "            current_input_idx = torch.tensor([start_token]).long().to(device).unsqueeze(0) # [1, 1]\n",
    "            \n",
    "            generated_ids = []\n",
    "            \n",
    "            for i in range(max_len):\n",
    "                word_emb = self.word_embedding(current_input_idx) # [1, 1, Embed_Dim]\n",
    "\n",
    "                lstm_input = torch.cat((word_emb, img_emb), dim=2) # [1, 1, Embed_Dim * 2]\n",
    "                lstm_out, hidden = self.lstm(lstm_input, hidden)\n",
    "                outputs = self.output_projection(lstm_out) # [1, 1, Vocab]\n",
    "                \n",
    "                probs = F.softmax(outputs[0, 0] / temperature, dim=0)\n",
    "                \n",
    "                if temperature == 0:\n",
    "                    next_word_idx = torch.argmax(probs).item()\n",
    "                else:\n",
    "                    next_word_idx = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                if next_word_idx == vocab.word2idx.get('<end>', 0):\n",
    "                    break\n",
    "                \n",
    "                generated_ids.append(next_word_idx)\n",
    "                current_input_idx = torch.tensor([next_word_idx]).long().to(device).unsqueeze(0)\n",
    "                \n",
    "            tokens = [vocab.idx2word.get(idx, '<unk>') for idx in generated_ids]\n",
    "            return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bef006dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 512\n",
    "HIDDEN_SIZE = 512\n",
    "IMG_FEAT_DIM = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e810f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m‚úÖ Pretrained model loaded from \u001b[1msaved_models/captioning_model_epoch_60.pth\u001b[0m\n",
      "üíª Device: \u001b[93mCPU\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "file_saved_model = 'saved_models/captioning_model_epoch_60.pth'\n",
    "\n",
    "checkpoint = torch.load(file_saved_model, map_location=DEVICE)\n",
    "model_captioning = CaptioningModel(VOCAB_SIZE, EMBED_DIM, IMG_FEAT_DIM, HIDDEN_SIZE).to(DEVICE)\n",
    "model_captioning.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_captioning = torch.optim.RMSprop(model_captioning.parameters(), lr=1e-4)\n",
    "optimizer_captioning.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "vocab.word2idx = checkpoint['vocab_word2idx']\n",
    "vocab.idx2word = checkpoint['vocab_idx2word']\n",
    "train_losses = checkpoint['train_losses']\n",
    "epoch = checkpoint['epoch']\n",
    "print(f\"{Colour.GREEN}‚úÖ Pretrained model loaded from {Colour.BOLD}{file_saved_model}{Colour.END}\")\n",
    "\n",
    "criterion_captioning = nn.CrossEntropyLoss(ignore_index=vocab.word2idx.get('<pad>', 0))\n",
    "\n",
    "print(f\"üíª Device: {Colour.YELLOW}{DEVICE.upper()}{Colour.END}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c8c98",
   "metadata": {},
   "source": [
    "# Webcam part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff6dc2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_webcam_demo(vocab, model_captioning, model_align, preprocess_align, device, temperature=0.3, refresh_rate=30):\n",
    "    model_captioning.eval() \n",
    "    \n",
    "    expected_dim = model_captioning.image_projection.in_features\n",
    "    feature_extractor_mode = \"CLIP\" if expected_dim == 512 else \"RESNET\"\n",
    "    \n",
    "    if feature_extractor_mode == \"CLIP\":\n",
    "        model_align.eval()\n",
    "\n",
    "    resnet_extractor = None\n",
    "    preprocess_resnet = None\n",
    "\n",
    "    if feature_extractor_mode == \"RESNET\":\n",
    "        print(\"‚è≥ Loading ResNet50 backbone for real-time extraction...\")\n",
    "        weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "        fasterrcnn_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights).to(device)\n",
    "        fasterrcnn_model.eval()\n",
    "        resnet_extractor = fasterrcnn_model.backbone.body\n",
    "                \n",
    "        preprocess_resnet = T.Compose([\n",
    "            T.Resize(256), T.CenterCrop(224), T.ToTensor(), \n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"{Colour.RED}‚ùå Error: could not open webcam.{Colour.END}\")\n",
    "        return\n",
    "\n",
    "    current_caption = \"waiting for video...\"\n",
    "    frame_count = 0\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    print(f\"{Colour.GREEN}‚úÖ Webcam started! Press 'q' to quit.{Colour.END}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            if frame_count % refresh_rate == 0:\n",
    "                with torch.no_grad():\n",
    "                    img_vector = None\n",
    "                    \n",
    "                    if feature_extractor_mode == \"CLIP\":\n",
    "                        img_tensor = preprocess_align(pil_img).unsqueeze(0).to(device)\n",
    "                        img_vector = model_align.encode_image(img_tensor).float()\n",
    "                    \n",
    "                    elif feature_extractor_mode == \"RESNET\":\n",
    "                        img_tensor = preprocess_resnet(pil_img).unsqueeze(0).to(device)\n",
    "                        backbone_outputs = resnet_extractor(img_tensor)\n",
    "                        feats = backbone_outputs['3']  # [1, 2048, H, W]\n",
    "                        img_vector = F.adaptive_avg_pool2d(feats, (1, 1)).flatten(start_dim=1)  # [1, 2048]\n",
    "                    \n",
    "                    if img_vector is not None:\n",
    "                        raw_caption = model_captioning.sample(img_vector, vocab, max_len=20, temperature=temperature, device=device)\n",
    "                        \n",
    "                        ignore = [\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"]\n",
    "                        clean_words = [w for w in raw_caption.split() if w not in ignore]\n",
    "                        current_caption = \" \".join(clean_words)\n",
    "                        \n",
    "                        current_caption = current_caption.capitalize()\n",
    "\n",
    "            h, w, _ = frame.shape\n",
    "            overlay = frame.copy()\n",
    "            cv2.rectangle(overlay, (0, h-60), (w, h), (0, 0, 0), -1)\n",
    "            alpha = 0.6\n",
    "            cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "            \n",
    "            text_size = cv2.getTextSize(current_caption, font, 0.8, 2)[0]\n",
    "            text_x = (w - text_size[0]) // 2\n",
    "            \n",
    "            cv2.putText(frame, current_caption, (text_x, h-20), font, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            cv2.imshow('Deep Learning Demo - Live Captioning', frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"{Colour.CYAN}üëã Demo closed.{Colour.END}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62323c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading ResNet50 backbone for real-time extraction...\n",
      "\u001b[92m‚úÖ Webcam started! Press 'q' to quit.\u001b[0m\n",
      "\u001b[96müëã Demo closed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run_webcam_demo(vocab, model_captioning, model_align, preprocess_align, DEVICE, temperature=0.3, refresh_rate=90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
